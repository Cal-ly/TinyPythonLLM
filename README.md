# ğŸ§  Python Project: `tinyllm_py`

A small-scale educational language model implementation in **Python + PyTorch**, built to mirror the structure and learning goals of the C# version.

---

## âš™ï¸ Training Configuration

### `TrainingConfig` (in `src/utils/config.py`)

Use `@dataclass` to define all hyperparameters:

```python
@dataclass
class TrainingConfig:
    epochs: int
    batch_size: int
    learning_rate: float
    max_context: int
    gradient_clip: float
    val_split: float
    sample_frequency: int
    temperature: float
    console_max_tokens: int
    console_temperature: float
    console_top_k: int
```

## ğŸ” Training Loop

### Core Loop (`src/training/train.py`)

* For each epoch:
  * Iterate over batches
  * Compute loss (`torch.nn.CrossEntropyLoss`)
  * Apply gradient clipping
  * Backpropagation (`loss.backward()`)
  * Optimizer step (`torch.optim.Adam`)
* Validation split for monitoring
* Sample generation at intervals
* Checkpointing and early stopping
* Save best model artifacts

## ğŸ® Interactive Console

### Features
* Real-time text generation
* Adjustable parameters (temperature, max tokens, top-k)
* Command interface for configuration
* Model loading from saved artifacts

### Usage
```bash
python scripts/console.py [model_directory]
```

## ğŸš€ Getting Started

1. **Train on Shakespeare dataset:**
   ```bash
   python scripts/train_shakespeare.py
   ```

2. **Launch interactive console:**
   ```bash
   python scripts/console.py
   ```

## ğŸ“ Project Structure

```
TinyPythonLLM/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ console/          # Interactive console
â”‚   â”œâ”€â”€ models/           # Transformer implementation
â”‚   â”œâ”€â”€ tokenization/     # Character tokenizer
â”‚   â”œâ”€â”€ training/         # Training utilities
â”‚   â””â”€â”€ utils/           # Configuration and logging
â”œâ”€â”€ scripts/             # Training and console scripts
â”œâ”€â”€ data/               # Training datasets
â””â”€â”€ logs/               # Training logs
```

---

## ğŸ“¦ Dependencies

**Minimum Python version**: `>=3.9`

Add the following to `requirements.txt`:

```txt
torch>=2.0
numpy>=1.23
tqdm>=4.65        # Optional, for progress bars
```

---

## ğŸ”¤ Tokenization

### `CharacterTokenizer`

* Maps each character to a unique integer and vice versa.
* Supports:

  * `fit(text: str)`
  * `encode(text: str) -> List[int]`
  * `decode(ids: List[int]) -> str`
  * `save_state(filepath)`
  * `load_state(filepath)`

Mirrors the C# `CharacterTokenizer` for interoperability and conceptual parity.

---

## ğŸ“š Dataset Handling

### Dataset Loader (in `src/training/data_loader.py`)

* Loads and preprocesses raw text:

  * Lowercasing
  * Whitespace normalization
* Splits data into training and validation sets.
* Supports:

  * Context windows
  * Batching (via `torch.utils.data.Dataset` + `DataLoader` or generator)

---

## ğŸ§  Model

### `TransformerModel` (in `src/models/`)

* Minimal transformer architecture:

  * Token embedding
  * Positional encoding
  * N transformer blocks
  * Multi-head attention
  * Feed-forward layers
  * Output projection
* Inspired by `ModelFactory.CreateTinyModel` (C# equivalent)
* Configurable via passed-in `TrainingConfig` or JSON

---

## âš™ï¸ Training Configuration

### `TrainingConfig` (in `src/utils/config.py`)

Use `@dataclass` to define all hyperparameters:

```python
@dataclass
class TrainingConfig:
    epochs: int
    batch_size: int
    learning_rate: float
    max_context: int
    gradient_clip: float
    val_split: float
    sample_frequency: int
    temperature: float
    console_max_tokens: int
    console_temperature: float
    console_top_k: int
```

## ğŸ” Training Loop

### Core Loop (`src/training/train.py`)

* For each epoch:
  * Iterate over batches
  * Compute loss (`torch.nn.CrossEntropyLoss`)
  * Apply gradient clipping
  * Backpropagation (`loss.backward()`)
  * Optimizer step (`torch.optim.Adam`)
* Validation split for monitoring
* Sample generation at intervals
* Checkpointing and early stopping
* Save best model artifacts

## ğŸ® Interactive Console

### Features
* Real-time text generation
* Adjustable parameters (temperature, max tokens, top-k)
* Command interface for configuration
* Model loading from saved artifacts

### Usage
```bash
python scripts/console.py [model_directory]
```

## ğŸš€ Getting Started

1. **Train on Shakespeare dataset:**
   ```bash
   python scripts/train_shakespeare.py
   ```

2. **Launch interactive console:**
   ```bash
   python scripts/console.py
   ```

## ğŸ“ Project Structure

```
TinyPythonLLM/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ console/          # Interactive console
â”‚   â”œâ”€â”€ models/           # Transformer implementation
â”‚   â”œâ”€â”€ tokenization/     # Character tokenizer
â”‚   â”œâ”€â”€ training/         # Training utilities
â”‚   â””â”€â”€ utils/           # Configuration and logging
â”œâ”€â”€ scripts/             # Training and console scripts
â”œâ”€â”€ data/               # Training datasets
â””â”€â”€ logs/               # Training logs
```

---
