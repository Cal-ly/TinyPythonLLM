# ğŸ§  TinyPythonLLM

A minimal educational language model implementation in **Python + PyTorch**, built to demonstrate transformer architecture and training principles.

## ğŸš€ Quick Start

### Option 1: Install as Package (Recommended)

```bash
# Clone the repository
git clone https://github.com/yourusername/TinyPythonLLM.git
cd TinyPythonLLM

# Install in development mode
pip install -e .

# Train a model
tinyllm-train data/shakespeare6k.txt

# Launch interactive console
tinyllm-console
```

### Option 2: Run Scripts Directly

```bash
# Clone the repository
git clone https://github.com/yourusername/TinyPythonLLM.git
cd TinyPythonLLM

# Install dependencies
pip install -r requirements.txt

# Train a model
python scripts/train.py data/shakespeare6k.txt

# Launch interactive console
python scripts/console.py
```

## ğŸ“¦ Installation

### Dependencies

**Minimum Python version**: `>=3.9`

Install dependencies:

```bash
pip install torch>=2.0 numpy>=1.23 tqdm>=4.65
```

Or from requirements.txt:

```bash
pip install -r requirements.txt
```

### Development Installation

For development, install in editable mode:

```bash
pip install -e .
```

This allows you to modify the code and see changes immediately without reinstalling.

## ğŸ¯ Usage Examples

### Training a Model

```bash
# Basic training
tinyllm-train data/shakespeare6k.txt

# Custom parameters
tinyllm-train data/shakespeare6k.txt --epochs 10 --batch_size 64 --learning_rate 1e-3

# Custom output directory
tinyllm-train data/shakespeare6k.txt --output_dir my_models
```

### Interactive Console

```bash
# Use default model location (trained_models/)
tinyllm-console

# Specify custom model directory
tinyllm-console path/to/my/model

# Or using Python directly
python -m tinyllm.console.interactive
```

### Programmatic Usage

```python
import torch
from tinyllm import Transformer, CharacterTokenizer, ModelConfig

# Load a trained model
from tinyllm.inference import load_model, generate_text

model, tokenizer = load_model("trained_models/shakespeare_model.pt")
generated = generate_text(model, tokenizer, "To be or not to be", max_tokens=100)
print(generated)
```

## âš™ï¸ Configuration

### Training Configuration

Customize training via `TrainingConfig`:

```python
from tinyllm.utils import TrainingConfig

config = TrainingConfig(
    batch_size=32,
    learning_rate=3e-4,
    max_epochs=10,
    sequence_length=256,
    d_model=512,
    num_heads=8,
    num_layers=6,
    dropout=0.1,
    device='cuda'  # or 'cpu'
)
```

### Model Architecture

Configure model via `ModelConfig`:

```python
from tinyllm.utils import ModelConfig

config = ModelConfig(
    vocab_size=65,  # Set automatically from data
    d_model=512,
    num_heads=8,
    num_layers=6,
    sequence_length=256,
    dropout=0.1
)
```

## ğŸ“ Project Structure

```
TinyPythonLLM/
â”œâ”€â”€ setup.py                 # Package installation
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ scripts/                # Backwards-compatible entry points
â”‚   â”œâ”€â”€ train.py            # Training script wrapper
â”‚   â””â”€â”€ console.py          # Console script wrapper
â”œâ”€â”€ src/                    # Main package source
â”‚   â”œâ”€â”€ __init__.py         # Package exports
â”‚   â”œâ”€â”€ console/            # Interactive console
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ interactive.py
â”‚   â”œâ”€â”€ inference/          # Text generation utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ generate.py
â”‚   â”œâ”€â”€ models/             # Transformer implementation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ transformer.py
â”‚   â”œâ”€â”€ scripts/            # CLI entry points
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ console.py
â”‚   â”œâ”€â”€ tokenization/       # Character tokenizer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ character_tokenizer.py
â”‚   â”œâ”€â”€ training/           # Training utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ data_loader.py
â”‚   â””â”€â”€ utils/              # Configuration and logging
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py
â”‚       â””â”€â”€ logger.py
â”œâ”€â”€ data/                   # Training datasets
â”œâ”€â”€ trained_models/         # Saved model checkpoints
â””â”€â”€ logs/                   # Training logs
```

## ğŸ”¤ Tokenization

The `CharacterTokenizer` maps each character to a unique integer:

```python
from tinyllm import CharacterTokenizer

tokenizer = CharacterTokenizer()
tokenizer.fit("Hello world!")

# Encode text to integers
tokens = tokenizer.encode("Hello")  # [72, 101, 108, 108, 111]

# Decode integers to text
text = tokenizer.decode([72, 101, 108, 108, 111])  # "Hello"
```

## ğŸ§  Model Architecture

The `Transformer` implements a minimal GPT-style architecture:

- **Token embedding**: Maps characters to dense vectors
- **Positional encoding**: Adds position information
- **Multi-head attention**: Allows tokens to attend to previous tokens
- **Feed-forward layers**: Non-linear transformations
- **Layer normalization**: Stabilizes training
- **Causal masking**: Ensures autoregressive generation

## ğŸ® Interactive Console Features

- **Real-time generation**: Type prompts and get completions
- **Adjustable parameters**: Change temperature and max tokens on the fly
- **Command interface**: Built-in help and configuration commands
- **Easy model switching**: Load different trained models

### Console Commands

```
/help      - Show available commands
/temp 0.8  - Set temperature (0.1-2.0)
/tokens 150 - Set max tokens (1-500)
/quit      - Exit console
```

## ğŸ”§ Troubleshooting

### Import Errors

If you get import errors:

1. **Install the package**: `pip install -e .`
2. **Check Python path**: Ensure you're in the project directory
3. **Use fallback scripts**: `python scripts/train.py` instead of `tinyllm-train`

### CUDA Issues

For GPU training:

```bash
# Check CUDA availability
python -c "import torch; print(torch.cuda.is_available())"

# Force CPU if needed
tinyllm-train data/shakespeare6k.txt --device cpu
```

### Path Issues

Models are saved to `trained_models/` by default. Ensure this directory exists or specify a custom path:

```bash
tinyllm-train data/shakespeare6k.txt --output_dir /path/to/models
tinyllm-console /path/to/models
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.